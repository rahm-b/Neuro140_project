{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oqupku5xtqdE",
        "outputId": "8224ead6-1739-4dd4-e1f9-551bec3d6a83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eeZGNsRvRR7L",
        "outputId": "dfb062d4-c449-469c-c1b3-ee87579fab11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PP6Ar5t8uDta",
        "outputId": "9fa4d2b7-dcb2-4076-a2b8-b0a0f9fb06c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# path in google drive where my chess text files are stored\n",
        "TEXT_FOLDER = \"/content/drive/MyDrive/chess_text_data/files/\"\n",
        "\n",
        "def load_all_texts(folder_path):\n",
        "    texts = []\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(\".txt\"):\n",
        "            with open(os.path.join(folder_path, filename), 'r', encoding='utf-8', errors='ignore') as f:\n",
        "                text = f.read()\n",
        "                texts.append(text)\n",
        "    return texts\n",
        "\n",
        "texts = load_all_texts(TEXT_FOLDER)\n",
        "print(f\"Num documents loaded: {len(texts)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSfe1d78uFSI",
        "outputId": "31f89823-18ee-4641-978a-fa3ed104d0f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 11 documents.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import chain\n",
        "\n",
        "# break up long text into smaller pieces\n",
        "def chunk_text(text, max_length=512):\n",
        "    \"\"\"Split a large text into smaller chunks of max_length tokens\"\"\"\n",
        "    tokenized = tokenizer(text, truncation=False, padding=False)\n",
        "    input_ids = tokenized['input_ids']\n",
        "\n",
        "    # split into chunks if text is longer than max_length\n",
        "    return [input_ids[i:i + max_length] for i in range(0, len(input_ids), max_length)]\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    # first chunk text into smaller chunks\n",
        "    all_chunks = list(chain(*[chunk_text(text) for text in examples[\"text\"]]))\n",
        "\n",
        "    # tokenize and label for MLM\n",
        "    encodings = tokenizer.pad({'input_ids': all_chunks}, padding=True)\n",
        "\n",
        "    labels = encodings[\"input_ids\"].copy()\n",
        "    for i in range(len(labels)):\n",
        "        labels[i] = [token if torch.rand(1).item() > 0.15 else -100 for token in labels[i]]\n",
        "\n",
        "    encodings[\"labels\"] = labels\n",
        "    return encodings"
      ],
      "metadata": {
        "id": "8XyOWBhdV_FC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "from transformers import BertTokenizerFast, BertForMaskedLM\n",
        "\n",
        "# load tokenizer and model\n",
        "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# split into 90% train, 10% validation\n",
        "train_texts, val_texts = train_test_split(texts, test_size=0.1, random_state=42)\n",
        "\n",
        "from datasets import Dataset\n",
        "\n",
        "train_dataset = Dataset.from_dict({'text': train_texts})\n",
        "val_dataset = Dataset.from_dict({'text': val_texts})\n",
        "\n",
        "tokenized_train = train_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
        "tokenized_val = val_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "# divide tokens into blocks of size 512\n",
        "block_size = 512\n",
        "\n",
        "def group_texts(examples):\n",
        "    concatenated = {k: sum(examples[k], []) for k in examples.keys()}\n",
        "    total_length = (len(concatenated[list(examples.keys())[0]]) // block_size) * block_size\n",
        "    result = {\n",
        "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
        "        for k, t in concatenated.items()\n",
        "    }\n",
        "    return result\n",
        "\n",
        "lm_train = tokenized_train.map(group_texts, batched=True)\n",
        "lm_val = tokenized_val.map(group_texts, batched=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254,
          "referenced_widgets": [
            "05e8c95b46e346febc3475376bdc0aa8",
            "d749094de8c64440a66b15d786f8c91d",
            "10b87fce16034b98affd3053363c9629",
            "53e00a3333ff4a72bfc3c320d842ad35",
            "b0dc89c1c50e48979809533dc9c909b1",
            "9f0304ced126411da9692562ccf03537",
            "1ae545edecdd4a3c8cc9d1f445acc421",
            "67d0a698081a45f8a02d8fe47a69ffdc",
            "ed4e1b00ff144dceac4c87180d0156ac",
            "9d73202e1a5a4c029e46c234c79b850f",
            "d26518135501468eaa1a8edccebdd6e1",
            "8ed5501e4d6241329c9a542dba8ae7f3",
            "ce79285ea54740f29c2698992c46cc3a",
            "b4656632443e458e8e816f3e6c0a6478",
            "68e1a22efd7f46cd8c16007beab11736",
            "97daccad631141108249b4bbb060927d",
            "5afb59d4b48f444897cb2046ab0463f6",
            "2f57233dab004527861a66a44281a170",
            "6e5c2fbb097d46afb9c5d8ca7f6263e1",
            "765a455a5dea474a9f3edf86791ea4b2",
            "0eaf6cc3248a4a2f8042f9221618bb5d",
            "cca4843f8517445e970b41d2323b1018",
            "5ae9d252601f4dd3a986deeb4e474d0c",
            "1383f23feaf642fc981512e668048db8",
            "c08b6ccfeeb04ebca22ea49cf091a064",
            "19fd216dbd0c48fda87b21121e4b4b36",
            "5640afcee6b24128be2de5ed4fa45d96",
            "45f5371b587a42d788b6a78b003032a0",
            "c9090084e37c49f1adc1b2cb5493bd0d",
            "ead350b55fd244caa1497d6816f2863e",
            "48b45252c2c24d4891e8fddf4ce3eb1d",
            "9ec654752aa241fea236599cb076b783",
            "565ee80346ee425fa73bbb4c2bb7f2a1",
            "a99aca10ae56471498c214ec6f71e75d",
            "ed2c916f3382465aa74f27455af66101",
            "33ced3c3e09648e4941a9cee3fca05ee",
            "cdf14924f401412c8322fcbafe675254",
            "6022aafcbd9840a6920c62a1bee3439d",
            "570361d698b04075ae72e1976c152b17",
            "16fe32082af6478486ed84509ed7d18d",
            "71e62f9e0e2d4ee38020968b6b95411f",
            "1bfb20a7efe84eaaa04380c488cf0065",
            "4df420a9737b4c849fc0d30fb9db6d0e",
            "8acde38b37db4e238747422ca2c1eb4d"
          ]
        },
        "id": "QHWiaFB7uIve",
        "outputId": "5c358756-9460-4e24-c3be-d22dbbde4307"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/9 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "05e8c95b46e346febc3475376bdc0aa8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (46961 > 512). Running this sequence through the model will result in indexing errors\n",
            "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8ed5501e4d6241329c9a542dba8ae7f3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/705 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5ae9d252601f4dd3a986deeb4e474d0c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/151 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a99aca10ae56471498c214ec6f71e75d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SOME CHECKS TO ENSURE CORRECT TOKENIZATION AND DIVISION INTO TRAIN AND VAL\n",
        "\n",
        "# check length of the first tokenized input\n",
        "print(len(tokenized_train[0]['input_ids']))\n",
        "\n",
        "# get number of tokens in the train and val dataset\n",
        "train_tokens_count = sum([len(example['input_ids']) for example in tokenized_train])\n",
        "val_tokens_count = sum([len(example['input_ids']) for example in tokenized_val])\n",
        "\n",
        "print(f\"Total tokens in train dataset: {train_tokens_count}\")\n",
        "print(f\"Total tokens in validation dataset: {val_tokens_count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9AUysaUWfv4",
        "outputId": "aec34083-f41e-4ef0-bbad-e67d07ad7636"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "512\n",
            "Total tokens in train dataset: 360960\n",
            "Total tokens in validation dataset: 77312\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "RgQp0klmNy5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForLanguageModeling, TrainingArguments\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=True,\n",
        "    mlm_probability=0.15\n",
        ")\n",
        "\n",
        "# directory in drive to save the model\n",
        "output_dir = \"/content/drive/MyDrive/chess-bert-mlm\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    overwrite_output_dir=True,\n",
        "    eval_strategy=\"epoch\",\n",
        "    num_train_epochs=20,\n",
        "    per_device_train_batch_size=8,\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    logging_steps=100,\n",
        "    learning_rate=3e-5,\n",
        "    weight_decay=0.01,\n",
        "    prediction_loss_only=True,\n",
        "    fp16=torch.cuda.is_available(),\n",
        ")"
      ],
      "metadata": {
        "id": "bQ3vSV5xuK3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=lm_train,\n",
        "    eval_dataset=lm_val,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 794
        },
        "id": "r0UFIXAPuQM4",
        "outputId": "cc36b717-a70c-4796-9e6b-eca351f7b40e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-34-39e38c68d57d>:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1780' max='1780' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1780/1780 09:50, Epoch 20/20]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.433528</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.748200</td>\n",
              "      <td>2.367669</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.350000</td>\n",
              "      <td>2.292325</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.219200</td>\n",
              "      <td>2.202871</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.123200</td>\n",
              "      <td>2.180906</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>2.063400</td>\n",
              "      <td>2.178564</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.968700</td>\n",
              "      <td>2.162492</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.933900</td>\n",
              "      <td>2.147320</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.891000</td>\n",
              "      <td>2.113254</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.891000</td>\n",
              "      <td>2.092436</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>1.866400</td>\n",
              "      <td>2.039525</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>1.823800</td>\n",
              "      <td>2.065657</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>1.770900</td>\n",
              "      <td>2.079249</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>1.781200</td>\n",
              "      <td>2.076987</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>1.753600</td>\n",
              "      <td>2.093477</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>1.756000</td>\n",
              "      <td>2.089989</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>1.712100</td>\n",
              "      <td>2.052424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>1.697900</td>\n",
              "      <td>2.065208</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>1.697900</td>\n",
              "      <td>2.083302</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.709600</td>\n",
              "      <td>2.059500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1780, training_loss=1.9394332800018654, metrics={'train_runtime': 591.1817, 'train_samples_per_second': 23.851, 'train_steps_per_second': 3.011, 'total_flos': 3711187860480000.0, 'train_loss': 1.9394332800018654, 'epoch': 20.0})"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_path = output_dir + \"/final_model\"\n",
        "trainer.save_model(final_path)\n",
        "tokenizer.save_pretrained(final_path)\n",
        "print(f\"Success, saved to {final_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8WlD8YWuU1J",
        "outputId": "55960f28-8075-4d74-f5c6-9fec03cd074a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to /content/drive/MyDrive/chess-bert-mlm/final_model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "fill_mask = pipeline(\"fill-mask\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "fill_mask(\"The most aggressive opening is the [MASK] defense.\")\n",
        "\n",
        "fill_mask(\"The [MASK] attack catches many novices off guard in chess.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WnJQCu5YfhVJ",
        "outputId": "769cfa52-2a7f-4f1d-9d5a-e776a54ec9e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.07478274405002594,\n",
              "  'token': 3313,\n",
              "  'token_str': 'double',\n",
              "  'sequence': 'the double attack catches many novices off guard in chess.'},\n",
              " {'score': 0.06943342834711075,\n",
              "  'token': 4474,\n",
              "  'token_str': 'surprise',\n",
              "  'sequence': 'the surprise attack catches many novices off guard in chess.'},\n",
              " {'score': 0.030931485816836357,\n",
              "  'token': 3722,\n",
              "  'token_str': 'simple',\n",
              "  'sequence': 'the simple attack catches many novices off guard in chess.'},\n",
              " {'score': 0.026560653001070023,\n",
              "  'token': 5000,\n",
              "  'token_str': 'knight',\n",
              "  'sequence': 'the knight attack catches many novices off guard in chess.'},\n",
              " {'score': 0.02193371020257473,\n",
              "  'token': 4525,\n",
              "  'token_str': 'resulting',\n",
              "  'sequence': 'the resulting attack catches many novices off guard in chess.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MLM pipeline\n",
        "mlm_pipeline = pipeline(\"fill-mask\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "example_sentence = \"The chess game started with the [MASK].\"\n",
        "predictions = mlm_pipeline(example_sentence)\n",
        "\n",
        "for prediction in predictions:\n",
        "    print(f\"Token: {prediction['token_str']} | Score: {prediction['score']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0uMa6xegtzb",
        "outputId": "ca5e6665-460a-4c7b-9535-7e5a1bb0cfa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token: diagram | Score: 0.11852438747882843\n",
            "Token: king | Score: 0.05426441878080368\n",
            "Token: queen | Score: 0.03489914909005165\n",
            "Token: game | Score: 0.02870725840330124\n",
            "Token: ending | Score: 0.024650724604725838\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test sentences for prediction\n",
        "test_sentences = [\n",
        "    \"The most aggressive opening is the [MASK] defense.\",\n",
        "    \"The [MASK] attack catches many novices off guard in chess.\",\n",
        "    \"The [MASK] is the most valuable chess piece.\",\n",
        "    \"The pawn can [MASK] the knight.\",\n",
        "    \"The knight moves to the [MASK] square on the chess board.\",\n",
        "    \"In chess, [MASK] plays first.\",\n",
        "    \"The pawn can [MASK] to a queen when it reaches the end of the chess board.\",\n",
        "    \"I think, therefore I [MASK].\",\n",
        "    \"I like [MASK] notation.\",\n",
        "    \"He likes to play [MASK] because he is smart.\"\n",
        "]\n",
        "\n",
        "for sentence in test_sentences:\n",
        "    predictions = mlm_pipeline(sentence)\n",
        "    print(f\"Input: {sentence}\")\n",
        "    for pred in predictions:\n",
        "        print(f\"Predicted Token: {pred['token_str']} | Score: {pred['score']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1-ODWLcgwFs",
        "outputId": "9964b63b-86b7-4575-9cf2-8cd5b1caf41f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: The most aggressive opening is the [MASK] defense.\n",
            "Predicted Token: sicilian | Score: 0.3536076247692108\n",
            "Predicted Token: indian | Score: 0.15814310312271118\n",
            "Predicted Token: french | Score: 0.09819328784942627\n",
            "Predicted Token: scandinavian | Score: 0.03905859217047691\n",
            "Predicted Token: american | Score: 0.018449973315000534\n",
            "Input: The [MASK] attack catches many novices off guard in chess.\n",
            "Predicted Token: double | Score: 0.07478274405002594\n",
            "Predicted Token: surprise | Score: 0.06943342834711075\n",
            "Predicted Token: simple | Score: 0.030931485816836357\n",
            "Predicted Token: knight | Score: 0.026560653001070023\n",
            "Predicted Token: resulting | Score: 0.02193371020257473\n",
            "Input: The [MASK] is the most valuable chess piece.\n",
            "Predicted Token: bishop | Score: 0.1851297914981842\n",
            "Predicted Token: knight | Score: 0.1105460450053215\n",
            "Predicted Token: rook | Score: 0.07221522927284241\n",
            "Predicted Token: queen | Score: 0.06810544431209564\n",
            "Predicted Token: king | Score: 0.05515346676111221\n",
            "Input: The pawn can [MASK] the knight.\n",
            "Predicted Token: attack | Score: 0.2449350506067276\n",
            "Predicted Token: capture | Score: 0.06962896138429642\n",
            "Predicted Token: follow | Score: 0.04860885068774223\n",
            "Predicted Token: take | Score: 0.044258859008550644\n",
            "Predicted Token: support | Score: 0.04029815271496773\n",
            "Input: The knight moves to the [MASK] square on the chess board.\n",
            "Predicted Token: last | Score: 0.13168324530124664\n",
            "Predicted Token: first | Score: 0.08840794861316681\n",
            "Predicted Token: next | Score: 0.05532427877187729\n",
            "Predicted Token: highest | Score: 0.04998132213950157\n",
            "Predicted Token: seventh | Score: 0.047321368008852005\n",
            "Input: In chess, [MASK] plays first.\n",
            "Predicted Token: he | Score: 0.5149043202400208\n",
            "Predicted Token: white | Score: 0.1148906871676445\n",
            "Predicted Token: she | Score: 0.08706346899271011\n",
            "Predicted Token: one | Score: 0.0777389258146286\n",
            "Predicted Token: black | Score: 0.030088327825069427\n",
            "Input: The pawn can [MASK] to a queen when it reaches the end of the chess board.\n",
            "Predicted Token: change | Score: 0.5094317197799683\n",
            "Predicted Token: turn | Score: 0.13393549621105194\n",
            "Predicted Token: switch | Score: 0.10189275443553925\n",
            "Predicted Token: shift | Score: 0.03748425096273422\n",
            "Predicted Token: transform | Score: 0.026168223470449448\n",
            "Input: I think, therefore I [MASK].\n",
            "Predicted Token: do | Score: 0.30303889513015747\n",
            "Predicted Token: am | Score: 0.06922060996294022\n",
            "Predicted Token: say | Score: 0.04231370985507965\n",
            "Predicted Token: think | Score: 0.0367627888917923\n",
            "Predicted Token: can | Score: 0.03269750252366066\n",
            "Input: I like [MASK] notation.\n",
            "Predicted Token: the | Score: 0.4304203987121582\n",
            "Predicted Token: this | Score: 0.09092862904071808\n",
            "Predicted Token: that | Score: 0.04924429953098297\n",
            "Predicted Token: your | Score: 0.03805304691195488\n",
            "Predicted Token: chess | Score: 0.02101488783955574\n",
            "Input: He likes to play [MASK] because he is smart.\n",
            "Predicted Token: chess | Score: 0.6446518898010254\n",
            "Predicted Token: games | Score: 0.1060619130730629\n",
            "Predicted Token: poker | Score: 0.021131010726094246\n",
            "Predicted Token: football | Score: 0.020561037585139275\n",
            "Predicted Token: golf | Score: 0.014021411538124084\n"
          ]
        }
      ]
    }
  ]
}
